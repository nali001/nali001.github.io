{"cells":[{"metadata":{"_uuid":"418c3fe230031ed49160fa94af4bde4b2f592544","_cell_guid":"6536eee5-e473-4036-a8d1-c942688da233"},"cell_type":"markdown","source":"Hello Kagglers!! Enjoying the competitions and Kernels? I bet you should be. Well, today I am gonna present you something totally new which is totally awesome. If you remember, inspired by the imperative style of **PyTorch**, **TF** developers introduced the **Eager mode** finally. Even though it is out, but I guess most of you wouldn't have tried it (at least on Kaggle Kernels). Yes, you guessed it correctly!! Today's kernel is to show how can you get PyTorch style and the combined power of TF  in Eager mode. **My reaction after using it?**\n![LEGENDARY!!](https://media.giphy.com/media/3ohzdIuqJoo8QdKlnW/giphy.gif)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"45fd8aa698a9b0fa66c76ad602e6cfb0600b4b44","_cell_guid":"3fb7991f-4113-4c06-a0e2-888217275bf6","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport os\nimport glob\nfrom pathlib import Path\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\ntfe.enable_eager_execution() #enable the eager mode before doing any operation\n\nfrom keras.preprocessing import image\nfrom skimage.io import imread, imsave, imshow\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(111)\ncolor = sns.color_palette()\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53625a2f6cef32630baa67b71c18a6926928529b","_cell_guid":"1a57b2b2-7b13-466a-a9ea-d0c43e2d79df"},"cell_type":"markdown","source":"FMNIST is a very cool dataset. MNIST, in my opinion, has become way too old to experiment and validate models. Replace MNIST with FMNIST!!  ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"56704104611a11e686f73ed54488c5d0e148cd6d","_cell_guid":"92b52b80-1d64-4f2b-94b9-0784d347f4c9","trusted":true},"cell_type":"code","source":"#Read the train and test csv first\ntrain = pd.read_csv('../input/fashion-mnist_train.csv')\ntest = pd.read_csv('../input/fashion-mnist_test.csv')\n\nprint(\"Number of training samples: \", len(train))\nprint(\"Number of test samples: \", len(test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3212e7328c27fb3c3551a6df174501b892e46fbc","_cell_guid":"5c1b97bd-21f8-4de9-8650-55cb4eb1538d","trusted":true},"cell_type":"code","source":"# Let's look at how the train dataset looks like\ntrain.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94969d664df98f7e866bdee3f75f4ce72a95bd60","_cell_guid":"074dac2d-d649-4254-999f-24cc2f275543"},"cell_type":"markdown","source":"So, for each sample, there are 785 columns out of which the first column represents the label of the corresponding sample and the other 784 columns are the pixel values of a (28x28) image. Now, let's look at the test dataset too","outputs":[],"execution_count":null},{"metadata":{"_uuid":"0fe0c4954204443e82dad8b4443b6e7ada4234f1","_cell_guid":"48b59135-7f05-4fa1-ba7d-16a4739e713f","trusted":true},"cell_type":"code","source":"# Random samples from test data\ntest.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adf0905e1bb0c3f376cc3c3aeedaff99be9bba0c","_cell_guid":"7411f3f8-b384-449c-8d11-af130d3aed99"},"cell_type":"markdown","source":"There are a total of **10 categories** in FMNIST dataset. I haven't read the description fully (LOL!!), so I will inspect the number of samples for each category myself. Let's do that first.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"dcd61cb88662805ecfa9a733e7b4a8e1b42f4b9d","_cell_guid":"9b21a881-8357-4e6b-8902-3cc2928e647f","trusted":true},"cell_type":"code","source":"# Get the count for each label\nlabel_count = train[\"label\"].value_counts()\n\n# Get total number of samples\ntotal_samples = len(train)\n\n# Make a dictionary for all the labels. \nlabels = {0 : \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\",\n         5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle Boot\"}\n\nfor i in range(len(label_count)):\n    label = labels[label_count.index[i]]\n    count = label_count.values[i]\n    pct = (count / total_samples) * 100\n    print(\"{:<15s}:   {} or {}%\".format(label, count, pct))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c6089ee52b2131916dd3f3a518f936af59677ca","_cell_guid":"48213bcd-d0c5-43ac-b23f-0d1f68156411"},"cell_type":"markdown","source":"Have I done something wrong here? Such a balanced dataset, so unrealistic!! Can't believe my eyes as I am seeing such a balanced dataset after such a long time. Let's quickly plot some samples for each category.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"0a6397983a501b9fd222237b73ffb46b99a8c2c1","_cell_guid":"bc603449-5ef0-44e3-bf88-b2266b2dadaa","trusted":true},"cell_type":"code","source":"# An empty list to collect some samples\nsample_images = []\n\n# Iterate over the keys of the labels dictionary defined in the above cell\nfor k in labels.keys():\n    # Get two samples for each category\n    samples = train[train[\"label\"] == k].head(2)\n    # Append the samples to the samples list\n    for j, s in enumerate(samples.values):\n        # First column contain labels, hence index should start from 1\n        img = np.array(samples.iloc[j, 1:]).reshape(28,28)\n        sample_images.append(img)\n        \nprint(\"Total number of sample images to plot: \", len(sample_images))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68239982399605b253fe7e9393a653dc563ec719","_cell_guid":"de20d9f8-b561-4f55-985f-789fdcc368d5","trusted":true},"cell_type":"code","source":"# Plot the sample images now\nf, ax = plt.subplots(5,4, figsize=(15,10))\n\nfor i, img in enumerate(sample_images):\n    ax[i//4, i%4].imshow(img, cmap='gray')\n    ax[i//4, i%4].axis('off')\nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9095deb78ff48d897e7f1665f4fca055f1c2707b","_cell_guid":"6fc8d6b8-080e-4b3d-a20c-89fb2613034d"},"cell_type":"markdown","source":"Ha!! Nice plot. Isn't it? \n\n## Preprocessing of data","outputs":[],"execution_count":null},{"metadata":{"_uuid":"97cd4a24a32acf543b69e458126a13e1050dec0b","_cell_guid":"67c2281d-ddca-49d8-88ce-c008866fc84c","trusted":true},"cell_type":"code","source":"# Separate the labels from train and test dataframe\ntr_labels = train[\"label\"]\nts_labels = test[\"label\"]\n\n# Drop the labels column from train dataframe as well as test dataframe\ntrain = train.drop([\"label\"], axis =1)\ntest = test.drop([\"label\"], axis=1)\n\n# Split the training dataset into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(train, tr_labels, test_size=0.2, random_state=111)\nprint(\"Number of samples in the train set: \", len(X_train))\nprint(\"Number of samples in the validation set: \", len(X_valid))\n\n# Just a consistency check\nprint(\"Train and validation shapes: \", end=\" \")\nprint(X_train.shape,y_train.shape,X_valid.shape, y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16318039b72745d1a4d18f2e7e67ba6dae7283d7","_cell_guid":"94419d06-448a-4c99-a0c1-8fedac7f67e5"},"cell_type":"markdown","source":"The pixel values in the dataset has been obtained after flattening the image pixels. There are 784 columns and each  image is  28x28 grayscale image. Let's reshape our data properly.  Also, there are 10 categories that we want to classify, we will use the `to_categorical` method available in keras for converting the labels to OHE ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"b28760aae9e6ba02d6a16cfd27e90e8d68924d66","_cell_guid":"350194a4-1a61-4807-bd3b-a6e777a1795c","trusted":true},"cell_type":"code","source":"# Reshape the data values\nX_train = np.array(X_train.iloc[:, :]).reshape(len(X_train),28,28,1)\nX_valid = np.array(X_valid.iloc[:, :]).reshape(len(X_valid), 28, 28,1)\nX_test = np.array(test.iloc[:,:]).reshape(len(test), 28, 28,1)\n\n# Some more preprocessing\nX_train = X_train.astype(np.float32)\nX_valid = X_valid.astype(np.float32)\nX_test = X_test.astype(np.float32)\n\ntrain_mean = X_train.mean()\n\n# Mean subtraction from pixels\nX_train -= train_mean\nX_valid -= train_mean\nX_test -= train_mean\n\n# Normalization\nX_train /=255.\nX_valid /=255.\nX_test /=255.\n\n# One Hot Encoding(OHE)\ny_train = to_categorical(y_train, num_classes=10).astype(np.int8)\ny_valid = to_categorical(y_valid, num_classes=10).astype(np.int8)\n\nprint(\"X_train shape: {}, y_train shape: {} \".format(X_train.shape, y_train.shape))\nprint(\"X_valid shape: {}, y_valid shape: {} \".format(X_valid.shape, y_valid.shape))\nprint(\"X_test shape: \", X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc041ab0e09d6f90a614e26709cbcec81fc715eb","_cell_guid":"10a79df5-fe40-4636-a918-2cdcfbab2873"},"cell_type":"markdown","source":"Great. Before moving to building blocks of our architecture, let's define a simple data generator for our model first","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"92fcf9e279af13165edda9a002f0faa6058fecc4","_cell_guid":"338f2102-e323-4d2d-b294-96794069eb26","trusted":true},"cell_type":"code","source":"# A simple data generator\ndef data_gen(data, labels, batch_size=8):\n    # Get total number of samples in the data\n    n = len(data)\n    \n    # Define two numpy arrays for containing batch data and labels\n    batch_data = np.zeros((batch_size, 28, 28, 1), dtype=np.float32)\n    batch_labels = np.zeros((batch_size,10), dtype=np.int8)\n    \n    # Get a numpy array of all the indices of the input data\n    indices = np.arange(n)\n    \n    # Initialize a counter\n    i =0\n    while True:\n        np.random.shuffle(indices)\n        # Get the next batch \n        next_batch = indices[(i*batch_size):(i+1)*batch_size]\n        for j, idx in enumerate(next_batch):\n            batch_data[j] = data[idx]\n            batch_labels[j] = labels[idx]\n        \n        yield batch_data, batch_labels\n        i +=1  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"543c6fae797c193538574569b40f2ced16f773c5","_cell_guid":"d6241173-920f-4d53-9031-bac1b5340584"},"cell_type":"markdown","source":"## Eager mode begins!! \n\nWait a second. What is **Eager execution** at all?\n\n(From Tensorflow docs)\n\nEager execution is a feature that makes TensorFlow execute operations immediately: concrete values are returned, instead of a computational graph to be executed later.\n\nAs a result, enabling eager execution provides:\n* A NumPy-like library for numerical computation with support for GPU acceleration and automatic differentiation.\n*  A flexible platform for machine learning research and experimentation.\n\nIt gives you an imperative way of defining your models. Now you may ask: **Why on the earth does it matter**? \nWell, there are a lot of reasons but the most simple one is this:\n\n***I want to write and execute everything as if I am writing a pure Python code, no tf.Session() and other things***\n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"43599f5a599bfa902ade7a70f6a57adc636aaab5","_cell_guid":"74def467-8f83-4fb0-ad40-ece0de5c0867"},"cell_type":"markdown","source":"### What is the best way to define a model?\n\nYou can define your model as you want. You could define your model inside a function or you could define it inside a class, it's totally up to you. If you ask me, I like to keep associated things together and this is where I love to write OOP. The following point summarizes how I have defined the model for this notebook.\n\n* Define a class FMNIST\n   * The constructor part (__init__): You should define all the layers that you are gonna use in your network here.  It is **highly recommended** to use high-level **tf.layers** API for defining your layers\n   * Other methods that like fit/predict, that you want to use for training and inference purposes. In the code below, I have defined just the predict function for the sake of simplicity here. You can define fit and predict both separately if you want.\n   \n * Define your cost function and your metric function(like accuracy, precision, etc) \n * Instantiate your model\n * Instantiate your optimizer\n * Define your gradient calculations (I will explain this later in the notebook)\n * Train and validate your network","outputs":[],"execution_count":null},{"metadata":{"_uuid":"9a1099dfe63f7691c62ed881f6f64c72e98a9983","_cell_guid":"7c31fc53-efb2-4c79-9203-a0780c540d00"},"cell_type":"markdown","source":"### Model","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"8b4895a9d0159e2de41a0ff5d95d8b9746baa054","_cell_guid":"b4d3538b-18b5-4bd1-903d-3e862c9ce8cd","trusted":true},"cell_type":"code","source":"# Class represnting our model\nclass FMNIST(object):\n    def __init__(self, data_format):\n        # Set the input shape according to the availability of GPU \n        if data_format == 'channels_first':\n            self._input_shape = [-1, 1, 28, 28]\n        else:\n            self._input_shape = [-1, 28, 28, 1]\n        \n        # Start defining the type of layers that you want in your network\n        self.conv1 = tf.layers.Conv2D(32, 3, \n                                      activation=tf.nn.relu, \n                                      padding='same', \n                                      data_format=data_format)\n        \n        self.maxpool = tf.layers.MaxPooling2D((2,2), (2,2), \n                                            padding='same', \n                                            data_format=data_format)\n        \n        self.conv2 = tf.layers.Conv2D(64, 3, \n                                      activation=tf.nn.relu, \n                                      padding='same', \n                                      data_format=data_format)\n        self.conv3 = tf.layers.Conv2D(128, 3, \n                                      activation=tf.nn.relu, \n                                      padding='same', \n                                      data_format=data_format)\n        \n        self.dense1 = tf.layers.Dense(1024, activation=tf.nn.relu)\n        self.dense2 = tf.layers.Dense(512, activation=tf.nn.relu)\n        self.dropout = tf.layers.Dropout(0.5)\n        self.dense3 = tf.layers.Dense(10)\n        \n        \n    #Combine the layers to form the architecture\n    def predict(self, inputs, drop=False):\n        x = tf.reshape(inputs, self._input_shape)\n        x = self.conv1(x)\n        x = self.maxpool(x)\n        x = self.conv2(x)\n        x = self.maxpool(x)\n        x = self.conv3(x)\n        x = self.maxpool(x)\n        x = tf.layers.flatten(x)\n        x = self.dense1(x)\n        x = self.dropout(x, training=drop) #enable at training and disable at testing\n        x = self.dense2(x)\n        x = self.dropout(x, training=drop)\n        x = self.dense3(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e738a41962f135b517f03d3a01deec12fc516521","_cell_guid":"5afa5328-2689-424e-b59a-3d012ffa17cb"},"cell_type":"markdown","source":"### Cost function/loss function","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"991678c75b9f5de572185842cec825158d941da6","_cell_guid":"8d3f258b-c86f-4eb6-a44c-58000a987973","trusted":true},"cell_type":"code","source":"# There are 10 categories, hence we will be using the cross-entropy loss here \ndef loss(model, inputs, targets, drop=False):\n    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n                          logits=model.predict(inputs, drop=drop), labels=targets))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2784c84aa057296b44e2770e2201cbef20653a89","_cell_guid":"d3df6b20-12ee-43f6-9370-2bd86aed7948"},"cell_type":"markdown","source":"### Metric calculation","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"fb593edf19c77f8046098f40a1d8da6f8a39463b","_cell_guid":"43e80e52-52e4-4bdb-98ba-642ce1bf2f62","trusted":true},"cell_type":"code","source":"# In our case, accuracy will be the metric that we are going to use for evaluation\ndef compute_accuracy(predictions, labels):\n    model_pred = tf.argmax(predictions, axis=1,output_type=tf.int64)\n    actual_labels = tf.argmax(labels, axis=1, output_type=tf.int64)\n    return tf.reduce_sum(tf.cast(tf.equal(model_pred, actual_labels),dtype=tf.float32)) / float(predictions.shape[0].value)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c51c158f9b36b1f57e1eb0926432de33febda0f","_cell_guid":"186e0251-7000-45e4-ac31-96d87a4f7fb5"},"cell_type":"markdown","source":"There are four things that are going on in the next cell.\n1.  Device selection: If GPU is there, data format should be NCHW as it is more optimized for GPU operations. If only CPU is there, the data format should be NHWC as it works better this way on CPU.\n\n2. Model instantiation\n\n3. Optimizer selection\n\n4. Gradient calculations: Although you can write your own function that calculates the gradient for each trainable variable for backpropagation but as the number of variables grows it can be hard to write one. The good thing is that TF provides implicit automatic differentiation. The only thing that you need to do is to pass your loss function name as a parameter to the `tfe.implicit_gradient()` method.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"bdfabffbf4e04d4d74c598e03a48be344fdbc0e9","_cell_guid":"eed05eb5-6e78-4633-ae31-6e9dd8986e48","trusted":true},"cell_type":"code","source":"# Device selection\ndevice = \"gpu:0\" if tfe.num_gpus() else \"cpu:0\"\n\n# Get an instance of your model\nmodel = FMNIST('channels_first' if tfe.num_gpus() else 'channels_last')\n\n# Define an optimizer\noptimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n\n# Automatic gradient calculation\ngrad = tfe.implicit_gradients(loss)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b925ca2d1360d54fe1b262e99b629e930e5994f","_cell_guid":"cc33f08c-37fb-4f44-8a8b-ea663344a0dc"},"cell_type":"markdown","source":" ### Training and Validation","outputs":[],"execution_count":null},{"metadata":{"_uuid":"732cf6905fe76f8d055f2c90cf361e8d26aab5ec","_cell_guid":"4dd8b3f7-b3b7-48d8-9242-d6333faca6af","trusted":true},"cell_type":"code","source":"# Define a batch size\nbatch_size = 32\n\n# Train data generator\ntrain_data_gen = data_gen(X_train, y_train)\n# Validation data generator\nvalid_data_gen = data_gen(X_valid, y_valid)\n\n# Get the number of batches\nnb_tr_batches = len(X_train) // batch_size\nnb_val_batches = len(X_valid) // batch_size\nprint(\"Number of train and validation batches: {}, {}\".format(nb_tr_batches, nb_val_batches))\n\n# Define number of epochs for which you want to train your model\nnb_epochs = 4\n\n# Train and validate\nfor i in range(nb_epochs):\n    print(\"\\n========== Epoch: {} ==============================\\n\".format(i+1))\n    with tf.device(device):\n        epoch_avg_loss = []\n        epoch_avg_acc = []\n        for j in range(nb_tr_batches):\n            inputs, targets = next(train_data_gen)\n            optimizer.apply_gradients(grad(model, inputs, targets))\n            if j % 500 == 0:\n                batch_loss = loss(model, inputs, targets, drop=True).numpy()\n                batch_acc = (compute_accuracy(model.predict(inputs, drop=True), targets).numpy())*100\n                epoch_avg_loss.append(batch_loss)\n                epoch_avg_acc.append(batch_acc)\n                print(\"Step {:<5s} ------> Loss: {:.4f}\".format(str(j), batch_loss))\n        \n        val_loss = loss(model, X_valid, y_valid).numpy()\n        val_acc = (compute_accuracy(model.predict(X_valid), y_valid).numpy())*100\n        print(\"\\ntrain_loss: {:.4f}  train_acc: {:.2f}%\".format(np.mean(epoch_avg_loss), np.mean(epoch_avg_acc)))        \n        print(\"val_loss: {:.4f}   val_acc: {:.2f}%\".format(val_loss, val_acc))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"842d8191a1e6e873c224a14a939d7b3afc22ed1f","_cell_guid":"f8c7eed4-57c4-4f9a-a698-7bdbda964d50"},"cell_type":"markdown","source":"I trained the model for 4 epochs only. You can train it for more. Also, I didn't put `EarlyStopping` here to put a check for overfitting, but you should definitely try it out.\n\n## Testing\n\nLet's do prediction on some of the test set samples","outputs":[],"execution_count":null},{"metadata":{"_uuid":"1bccd93ae8e86be1e3f2cef7b4d668c69e1edfaf","_cell_guid":"f6cac392-6f94-4605-9c7e-6e627d87997a","trusted":true},"cell_type":"code","source":"test_samples = X_test[:10]\ntest_labels = ts_labels[:10]\nprint(test_samples.shape, test_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65dab75884021354b058a3f04b5ef7daf59501ea","_cell_guid":"888b3def-6ddf-4cca-b0e3-3f116eae9e1b","trusted":true},"cell_type":"code","source":"prob = model.predict(inputs=test_samples).numpy()\npredicted_labels = list(tf.argmax(prob, axis=1,output_type=tf.int64).numpy())\nprint(\"True labels:     \", test_labels.tolist())\nprint(\"Predicted labels: \", predicted_labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbf3bb0174ffdc9d1635da0569cfb6e9d0dc3fb8","_cell_guid":"3d440cbb-95ef-4bb9-9f72-308e7c6fe8f6"},"cell_type":"markdown","source":"Pretty good results. Ha!!","outputs":[],"execution_count":null},{"metadata":{"_uuid":"aa7cc1b34f77106d3ff5cfbe6568fc7edc20179d","_cell_guid":"eae548a9-607f-4685-b3ae-6a802663985d","trusted":true},"cell_type":"code","source":"# Let's visualize the results\nf, ax = plt.subplots(2,5, figsize=(20,5))\n\nfor i in range(10):\n    img = X_test[i].reshape(28,28)\n    ax[i//5, i%5].imshow(img)\n    ax[i//5, i%5].axis('off')\n    ax[i//5, i%5].set_title(labels[predicted_labels[i]])\nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d22e84d309a8ed385532bb4ea2e5533413befa2","_cell_guid":"27f4d0a3-9bc8-43a0-87f1-6a8d32b6a0b4"},"cell_type":"markdown","source":"That's it folks!! As you can see, **Eager** is the way to go and I think TF guys are working hard to make it happen. That being said, **Eager mode** is still in beta and requires a lot of optimization. Hopefully, the next version of TF will ship with awesome functionalities. \n\n### TIPS:\n1. Integrate **Dataset API** in this code, to get a much better generator that can do data augmentation on the fly.\n2. Copy this notebook and run on Google Colab\n3. Compare the execution speed of plain TF with code for eager mode\n4. Train a **GAN** using eager execution\n\nIf you want me to do some of these in the Kaggle Kernels, let me know about it in the comments section. If you find anything wrong, or if you want to provide some suggestion, please do in the comments section below. Also, **please upvote if you liked the kernel**","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"974ef7685726f38b7e0a2d1063de74876a8df642","_cell_guid":"84dc3dcd-44ba-4d67-a489-76091e024ea5","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}