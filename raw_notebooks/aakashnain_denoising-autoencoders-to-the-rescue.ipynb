{"cells":[{"source":"Hello Kagglers!! This week was totally boring, so in order to cheer up myself, I wanted to put up a new kernel with some cool stuff. I know ..I know...I know autoencoders aren't something new but for some people, they are  a big deal, especially for people new to the field of deep learning. Well, without saying anything further, let's dive in and try to build an autoencoder in Keras.\n![Autoenoders, hell yeah!!](https://media.giphy.com/media/NL6i0bK8omoMM/giphy.gif)","metadata":{},"cell_type":"markdown"},{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport os\nfrom pathlib import Path\nimport glob\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom skimage.io import imread, imshow, imsave\nfrom keras.preprocessing.image import load_img, array_to_img, img_to_array\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Input\nfrom keras.optimizers import SGD, Adam, Adadelta, Adagrad\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nnp.random.seed(111)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"75b31c47b7ef9b8483af2b015a610c878e4e4bd8","_cell_guid":"6a4cf509-d694-4bc9-9516-ee13bc8a9bf3"},"cell_type":"code","outputs":[],"execution_count":1},{"source":"# Define paths in the fancy way, after all we have pathlib now. No more os.path.join...whatever!!\ninput_dir  = Path('../input/')\ntrain = input_dir / 'train'\ntrain_cleaned = input_dir / 'train_cleaned'\ntest = input_dir / 'test'","metadata":{"collapsed":true},"cell_type":"code","outputs":[],"execution_count":2},{"source":"# The train directory comtaims png files. Let's get all the files and check a few samples\ntrain_images = sorted(os.listdir(train))\ntrain_labels = sorted(os.listdir(train_cleaned))\ntest_images = sorted(os.listdir(test))\nprint(\"Total number of images in the training set: \", len(train_images))\nprint(\"Total number of cleaned images found: \", len(train_labels))\nprint(\"Total number of samples in the test set: \", len(test_images))\n\n# Lets' plot a few samples\n# First row will be raw data, second row will be the corresponding cleaned images\nsamples = train_images[:3] + train_labels[:3]\n\nf, ax = plt.subplots(2, 3, figsize=(20,10))\nfor i, img in enumerate(samples):\n    img = imread(train/img)\n    ax[i//3, i%3].imshow(img, cmap='gray')\n    ax[i//3, i%3].axis('off')\nplt.show()    ","metadata":{},"cell_type":"code","outputs":[],"execution_count":3},{"source":"# Lets' define our autoencoder now\ndef build_autoenocder():\n    input_img = Input(shape=(420,540,1), name='image_input')\n    \n    #enoder \n    x = Conv2D(32, (3,3), activation='relu', padding='same', name='Conv1')(input_img)\n    x = MaxPooling2D((2,2), padding='same', name='pool1')(x)\n    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv2')(x)\n    x = MaxPooling2D((2,2), padding='same', name='pool2')(x)\n    \n    #decoder\n    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv3')(x)\n    x = UpSampling2D((2,2), name='upsample1')(x)\n    x = Conv2D(32, (3,3), activation='relu', padding='same', name='Conv4')(x)\n    x = UpSampling2D((2,2), name='upsample2')(x)\n    x = Conv2D(1, (3,3), activation='sigmoid', padding='same', name='Conv5')(x)\n    \n    #model\n    autoencoder = Model(inputs=input_img, outputs=x)\n    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n    return autoencoder","metadata":{"collapsed":true},"cell_type":"code","outputs":[],"execution_count":4},{"source":"autoencoder = build_autoenocder()\nautoencoder.summary()","metadata":{},"cell_type":"code","outputs":[],"execution_count":5},{"source":"The dataset is small, so we can actually store the numpy arrays of images and corresponding cleaned images numpy arrays into two numpy arrays (Ha!! a lot of `numpy` and `array` in a single sentence). This would eliminate the need for defining a generator. ","metadata":{},"cell_type":"markdown"},{"source":"X = []\nY = []\n\nfor img in train_images:\n    img = load_img(train / img, grayscale=True,target_size=(420,540))\n    img = img_to_array(img).astype('float32')/255.\n    X.append(img)\n\nfor img in train_labels:\n    img = load_img(train_cleaned / img, grayscale=True,target_size=(420,540))\n    img = img_to_array(img).astype('float32')/255.\n    Y.append(img)\n\n\nX = np.array(X)\nY = np.array(Y)\n\nprint(\"Size of X : \", X.shape)\nprint(\"Size of Y : \", Y.shape)","metadata":{},"cell_type":"code","outputs":[],"execution_count":27},{"source":"# Split the dataset into training and validation. Always set the random state!!\nX_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.1, random_state=111)\nprint(\"Total number of training samples: \", X_train.shape)\nprint(\"Total number of validation samples: \", X_valid.shape)","metadata":{},"cell_type":"code","outputs":[],"execution_count":28},{"source":"# Train your model\nautoencoder.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_valid, y_valid))","metadata":{},"cell_type":"code","outputs":[],"execution_count":29},{"source":"# Let's test our model on a sample test image\nsample_test = load_img(test/ test_images[10], grayscale=True, target_size=(420,540))\nsample_test = img_to_array(sample_test)\nsample_test_img = sample_test.astype('float32')/255.\nsample_test_img = np.expand_dims(sample_test, axis=0)\n\n# Get the predition\npredicted_label = np.squeeze(autoencoder.predict(sample_test_img))\n\nf, ax = plt.subplots(1,2, figsize=(10,8))\nax[0].imshow(np.squeeze(sample_test), cmap='gray')\nax[1].imshow(np.squeeze(predicted_label.astype('int8')), cmap='gray')\nplt.show()","metadata":{},"cell_type":"code","outputs":[],"execution_count":40},{"source":"Not bad!! I just trained this model for 10 epochs becuase Kaggle kernels don't come with a GPU. But you can see that it's clearly going in the right direction. \n\n### Tips:\n* Download the dataset, upload to your Gdrive. Use Colab...FREE GPU!!!\n* Make a **deeeppeerr** model\n* Avoid Adam, it's too aggressive for this task (I might be wrong)\n\n\nUpvote the kernel if you liked it. Also, if you found anything wrong in the notebook or if you want to suggest something as an improvement, please do share that in the comments section. I hope you enjoyed the kernel.","metadata":{},"cell_type":"markdown"},{"source":"","metadata":{"collapsed":true},"cell_type":"code","outputs":[],"execution_count":null}],"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"version":"3.6.4","nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}}}