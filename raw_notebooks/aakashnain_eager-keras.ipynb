{"cells":[{"metadata":{"_uuid":"c13a7b573be8549f10bde7c147f2c33628434cf2","_cell_guid":"36a347cd-4333-45e1-bb59-f095981492f5"},"cell_type":"markdown","source":"Hello everyone. I am gonna do something very crazy today. In my last [kernel](https://www.kaggle.com/aakashnain/eagerfmnist) on Eager mode in tensorflow, people asked me to do a kernel on the [tf.dataset API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). Well, I actually wanted to do one but doing a simple kernel on dataset API wasn't something motivating to invest my time on a kernel. So, I thought about it and then I came up with a thought to show that how can you mix `Keras`, `Eager`, `TF` and `tf.data.Dataset` API, all in one single model!! \n\nI am gonna show a lot of cool stuff today(it was cool for me atleast :P). It's a bit long kernel as compared to others, so hold tight. I hope this will kernel will help fine grain your understanding on the above four things. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport os\nimport cv2\nimport glob\nimport shutil\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mimg\nfrom os import listdir, makedirs, getcwd, remove\nfrom os.path import isfile, join, abspath, exists, isdir, expanduser\nfrom PIL import Image\nfrom pathlib import Path\nfrom skimage.io import imread, imsave\nfrom skimage.transform import resize, rescale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\ncolor = sns.color_palette()\n%matplotlib inline\nnp.random.seed(111)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"239266ebbab65a25392a59182d109493841fe9ad","_cell_guid":"e8eae73c-56eb-4059-aef9-23e15ac8bd81"},"cell_type":"markdown","source":"I have added VGG16 weights as the external data source to this kernel because we will be doing transfer learning in the end. "},{"metadata":{"_uuid":"48c4bdae44c156f7093e89035ccaddf4faad9c48","_cell_guid":"b8b42d18-884a-4efb-9aa4-6d14a05c2e6e","trusted":true},"cell_type":"code","source":"# Import tensorflow \nimport tensorflow as tf\n# Enable eager execution\nimport tensorflow.contrib.eager as tfe\n# I will explain this thing in the end\ntf.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)\n# Import keras but from inside tf \nfrom tensorflow import keras","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"c34458e8da775abad4a41f3519620c50d1c41b6a","_cell_guid":"543dab4d-d58c-49f5-9e2c-c13519562cd2","trusted":true},"cell_type":"code","source":"# We can check if we are executing in eager mode or not. This code should print true if we are in eager mode.\ntf.executing_eagerly()","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"9953df57c53d9b6f95ca6c8bfb9c5f8b0af3feb8","_cell_guid":"23e95c4f-c652-42ac-abf8-abc2eac12d74"},"cell_type":"markdown","source":"We will make a directory  to store VGG weight for keras. I have already shown how to do this  in one of my kernels. If you want to get more details on this step, have a look at [this](https://www.kaggle.com/aakashnain/flowers-are-mesmerizing)"},{"metadata":{"_uuid":"fbd17bc1b68305f1242c114133f507c82e99ac3b","collapsed":true,"_cell_guid":"bc39da6f-b6f4-4de0-80b8-aa6d21c2fdbe","trusted":true},"cell_type":"code","source":"# Check for the directory and if it doesn't exist, make one.\ncache_dir = expanduser(join('~', '.keras'))\nif not exists(cache_dir):\n    makedirs(cache_dir)\n    \n# make the models sub-directory\nmodels_dir = join(cache_dir, 'models')\nif not exists(models_dir):\n    makedirs(models_dir)","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"daf9e0d8780199a7186ded419914d81e9b5e1396","collapsed":true,"_cell_guid":"346ada2b-5b40-4fdc-9cbb-7b12e5cbf30b","trusted":true},"cell_type":"code","source":"# Copy the weight to the .keras/models directory\n!cp ../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 ~/.keras/models/","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"624c33b6989142a2656e53d9e4eff82fc53a821a","collapsed":true,"_cell_guid":"eb0359fc-d303-4a3e-b4d0-e99a9e9dba34","trusted":true},"cell_type":"code","source":"# As usual, define some paths first to make life simpler\ntraining_data = Path('../input/10-monkey-species/training/training/') \nvalidation_data = Path('../input/10-monkey-species/validation/validation/') \nlabels_path = Path('../input/10-monkey-species/monkey_labels.txt')","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"4b91d4b6a70f00798ef6fe7d51df6ce814961790","_cell_guid":"9bda0757-f15a-4db9-85d5-bc272586e215"},"cell_type":"markdown","source":"Information about the labels are given in the `monkey_labels.txt` file. This file contains `label, latin name, common name, corresponding train images count, corresponding validation images count`. Let's read this file and store this information in a pandas dataframe."},{"metadata":{"_uuid":"824fe6ace4a928134a4b3898cbe1203a3d587a77"},"cell_type":"markdown","source":"## Simple analysis"},{"metadata":{"_uuid":"58dddd4e710e8058346d5ae3de05d2407f188538","_cell_guid":"1fe76922-54fd-4a3c-8e4c-e0bff31c0a51","trusted":true,"collapsed":true},"cell_type":"code","source":"# How many categories are there in the dataset?\nlabels_info = []\n\n# Read the file\nlines = labels_path.read_text().strip().splitlines()[1:]\nfor line in lines:\n    line = line.split(',')\n    line = [x.strip(' \\n\\t\\r') for x in line]\n    line[3], line[4] = int(line[3]), int(line[4])\n    line = tuple(line)\n    labels_info.append(line)\n    \nlabels_info = pd.DataFrame(labels_info, columns=['Label', 'Latin Name', 'Common Name', 'Train Images', 'Validation Images'], index=None)\nlabels_info.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"076e0cb1fdea57125e96b2b997573a72f5d4052d","_cell_guid":"9ca5120d-3623-4463-850d-a0eb4a60f799"},"cell_type":"markdown","source":"You can see from the above output that the number of images for each category is almost similar which is a good thing this almost a balanced dataset. We can plot this information too."},{"metadata":{"_uuid":"9a5318e7d9153674e9d8577a3de80aa2dae505a5","_cell_guid":"90caee3b-808f-43b8-9dfe-c365e8c13adf","trusted":true},"cell_type":"code","source":"labels_info.plot(x='Label', y=['Train Images','Validation Images'], kind='bar', figsize=(20,5))\nplt.ylabel('Count of images')\nplt.show()","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"1238a40c23c62809a76c123037ee4327ebabad61","_cell_guid":"acd761d0-6d9d-41de-8877-203d8b3ed3be"},"cell_type":"markdown","source":"We will be training our model on random batches of the training dataset.  As the training images are already provided in separate sub-directories, `keras ImageDataGenerator` would have been a perfect choice but as we are going to use the `tf.data.Dataset` API, we will read the file names and corresponding labels and we will store them in a pandas dataframe."},{"metadata":{"_uuid":"fb02e0828c2106f4a128fedf198d26ef40326d27"},"cell_type":"markdown","source":"## Preprocessing "},{"metadata":{"_uuid":"8a4eb45403e38ccdbb9b93c73d2057df1a566d5f","_cell_guid":"2de15369-4568-452e-9854-4068bb52d480","trusted":true},"cell_type":"code","source":"# Creating a dataframe for the training dataset\ntrain_df = []\nfor folder in os.listdir(training_data):\n    # Define the path to the images\n    imgs_path = training_data / folder\n    # Get the list of all the images stored in that directory\n    imgs = sorted(imgs_path.glob('*.jpg'))\n    print(\"Total number of training images found in the directory {}: {}\".format(folder, len(imgs)))\n    # Append the info to out list for training data \n    for img_name in imgs:\n        train_df.append((str(img_name), folder))\n\ntrain_df = pd.DataFrame(train_df, columns=['image', 'label'], index=None)        \nprint(\"\\n\",train_df.head(10), \"\\n\")\nprint(\"=================================================================\\n\")\n\n\n# Creating dataframe for validation data in a similar fashion\nvalid_df = []\nfor folder in os.listdir(validation_data):\n    imgs_path = validation_data / folder\n    imgs = sorted(imgs_path.glob('*.jpg'))\n    print(\"Total number of validation images found in the directory {}: {}\".format(folder, len(imgs)))\n    for img_name in imgs:\n        valid_df.append((str(img_name), folder))\n\nvalid_df = pd.DataFrame(valid_df, columns=['image', 'label'], index=None)        \nprint(\"\\n\", valid_df.head(10))","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"b2bd68b9f455e5a49cf35bb2314f74b424697d1f","_cell_guid":"7f0239ae-b6d8-436f-ac29-15b23e5686f0","trusted":true},"cell_type":"code","source":"# Shuffle the train and validation dataframes\ntrain_df = train_df.sample(frac=1.).reset_index(drop=True)\nvalid_df = valid_df.sample(frac=1.).reset_index(drop=True)\n\nprint(\"Total number of training samples: \", len(train_df))\nprint(\"Total number of validation samples: \", len(valid_df))","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"6e6885f0cfb95eeb1019a802cafd1c2cc2402ed4","collapsed":true,"_cell_guid":"fdc99c8d-98e8-47ac-a95c-861ace275aa9","trusted":true},"cell_type":"code","source":"# Create a dictionary to map the labels to integers\nlabels_dict= {'n0':1, 'n1':1, 'n2':2, 'n3':3, 'n4':4, 'n5':5, 'n6':6, 'n7':7, 'n8':8, 'n9':9}","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d593e509809658d937b4bd2040cb8087c1d41b7"},"cell_type":"code","source":"# Let's look at some sample images first\nsample_images = []\nsample_train_images_df = train_df[:12]\nf,ax = plt.subplots(4,3, figsize=(30,30))\nfor i in range(12):\n    img = cv2.imread(sample_train_images_df.loc[i, 'image'])\n    img = cv2.resize(img, (224,224))\n    inv_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    lab = labels_info.loc[labels_info['Label'] == sample_train_images_df.loc[i, 'label']]['Common Name'].values[0]\n    sample_images.append(img)\n    ax[i//3, i%3].imshow(inv_img)\n    ax[i//3, i%3].set_title(lab, fontsize=14)\n    ax[i//3, i%3].axis('off')\n    ax[i//3, i%3].set_aspect('auto')\nplt.show() ","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"c81df36501af042b2566be05218fe9c2dc468a06","_cell_guid":"2eb5b4ae-2d65-4ea9-bd78-351b816326b1"},"cell_type":"markdown","source":"Now, it's time to define our batch data generator using tf.Dataset API. There are many ways in which you can intialize an iterator instance of the `tf.data.Dataset` API. For example:\n* You can read tensor slices from numpy arrays, given that your data can fit in the memory\n* You can read tensor slices from TFRecords\n* You can read tensor slices from filenames, etc.\n\nI highly recommend going over the documentation over [reading inputs](https://www.tensorflow.org/programmers_guide/datasets#reading_input_data). This will help you understand how you should initialize your input pipeline for your model.\n\nA question that you must ask: **Why use this API? Why not simply define a python generator ?**\nThis is a very good question. When you train your model, you read data from disk in batches and then the data is transferred to GPU for training. Plus GPU operates much faster and do heavily parallized operations as compared to CPU.  Hence most of the time, GPU sits idle as processing data on CPU and transferring it to GPU after ward is a bottleneck. The `tf.data.Dataset` API tries to overcome this limitation by processing data in a much more efficient way. It follows the ETL methodology. I can give more details about it but it would be better if you just watch [this](https://youtu.be/uIcqeP7MFH0?list=PLQY2H8rRoyvxjVx3zfw4vA4cvlKogyLNN) video from tf summit. **I highly recommend it.**\n\nP.S: There are a lot of features that are still not documented properly on the TensorFlow page. I spent a lot of time figuring out the best way to use it but phew!!!! Similarly, the documentation for eager mode is a bit sparse at this moment but nothing is stopping you to experiment with all these things."},{"metadata":{"_uuid":"20da1669ba80b3fa111a5a1d3af49c80bfbe215d","collapsed":true,"_cell_guid":"f53d202d-4159-4115-b85a-b9ed89461cdb","trusted":true},"cell_type":"code","source":"# A data generator using tf dataset\ndef data_gen(X=None, y=None, batch_size=32, nb_epochs=1):\n    # A simple function to decode an image\n    # You can use cv2 also but for that the syntax is a little bit ugly this time, so we will use this as of now\n    def _parse_function(filename, label):\n        image_string = tf.read_file(filename)\n        image_decoded = tf.image.decode_jpeg(image_string)\n        # We will resize each image to 224x224x3\n        image_resized = tf.cast(tf.image.resize_images(image_decoded, [224, 224]), dtype=tf.float32)\n        #Normalize the pixel values\n        image_resized = tf.truediv(image_resized, 255.)\n        # One hot encoding of labels\n        label = tf.one_hot(label, depth=10, dtype=tf.float32)\n        return image_resized, label\n    \n    # We will read tensor slices from filenames and labels\n    dataset = tf.data.Dataset.from_tensor_slices((X,y))\n    # This num_parallel_calls is for executing this functions on multiple cores\n    dataset = dataset.map(_parse_function,num_parallel_calls=32)\n    dataset = dataset.batch(batch_size)#.repeat(nb_epochs)\n    return dataset","execution_count":61,"outputs":[]},{"metadata":{"_uuid":"cbe6d474507c306d74762969d96b85c03aef5122"},"cell_type":"markdown","source":"In order to use the dataset, we will get a list of all image files and a corresponding list of image labels. "},{"metadata":{"_uuid":"10ddb903b9a70c04f4c37a313a6dffbb99e75f3c","collapsed":true,"_cell_guid":"19bedfba-1d8d-4781-a318-c3be735df02c","trusted":true},"cell_type":"code","source":"# Get a list of all the images and the corresponding labels in the training data\ntrain_images = tf.constant(train_df['image'].values)\ntrain_labels = tf.constant([labels_dict[l] for l in train_df['label'].values])\n\n# Do the same for the validation data\nvalid_images = tf.constant(valid_df['image'].values)\nvalid_labels = tf.constant([labels_dict[l] for l in valid_df['label'].values])","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"4c6c9c3da4e2d383451cd2e31d5dcf7af88b3e7c"},"cell_type":"markdown","source":"We will start defining our model now. We will use the VGG16 pre trained network as our base network(up to conv block) and then add some dense layers on the top of that. As it's transfer learning, we will freeze the weights of the base network and will train our newly added layers only."},{"metadata":{"_uuid":"091fb33ed2917c950351fc8a5b3002c8ebb8d632"},"cell_type":"markdown","source":"## Model"},{"metadata":{"_uuid":"394474572cde56600766c01344a6c76bc2fbb360","collapsed":true,"_cell_guid":"14e1f6c5-62c2-4fdf-b69b-0ec639bcb11d","trusted":true},"cell_type":"code","source":"# Model definition\ndef build_model():\n    base_model = keras.applications.VGG16(input_shape=(224,224,3), include_top=False, weights='imagenet')\n    base_model_output = base_model.output\n    x = keras.layers.Flatten(name='flatten')(base_model_output)\n    x = keras.layers.Dense(1024, activation='relu', name='fc1')(x)\n    x = keras.layers.Dropout(0.5, name='dropout1')(x)\n    x = keras.layers.Dense(512, activation='relu', name='fc2')(x)\n    x = keras.layers.Dropout(0.5, name='dropout2')(x)\n    out = keras.layers.Dense(10, name='output', activation='softmax')(x)\n    \n    for layer in base_model.layers:\n        layer.trainable = False\n\n    model = keras.models.Model(inputs=base_model.input, outputs=out)\n    return model","execution_count":57,"outputs":[]},{"metadata":{"_uuid":"e77e6f3ba02a13db72c089555cc597a126b8ffac","_cell_guid":"ba85579f-d2ae-43ed-81e9-8cd6b5dea945","trusted":true},"cell_type":"code","source":"model = build_model()\nmodel.summary()","execution_count":63,"outputs":[]},{"metadata":{"_uuid":"2e9d9c5b7bdfb52942f9a503473a3f4e273862bf","collapsed":true,"_cell_guid":"987ba663-639b-4098-af5d-d3765b7db317","trusted":true},"cell_type":"code","source":"# Compile the model\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=tf.train.RMSPropOptimizer(learning_rate=0.001))","execution_count":64,"outputs":[]},{"metadata":{"_uuid":"bdb65778e969a3115e841a19654b2bb02543ba3e"},"cell_type":"markdown","source":"We are all set to train our model. We require to do the following things:\n* Define batch size\n* Define number of epochs\n* Get an instance of our data generator\n* Train model on the incoming batch\n* Calculate  batch loss and batch accurcay"},{"metadata":{"_uuid":"5e9582407354227118e21974ddc5e20658902928"},"cell_type":"markdown","source":"## Training and Evaluation"},{"metadata":{"_uuid":"991c89f839d6e72c861f597ab1b954a242690b60","_cell_guid":"e6fa87b0-16b5-4456-a012-781c228c2f60","trusted":true},"cell_type":"code","source":"# Number of epochs for which you want to train your model\nnb_epochs = 2\n\n# Define a batch size you want to use for training\nbatch_size=32\n\n# Get an instance of your iterator\ntrain_gen = data_gen(X=train_images, y=train_labels, batch_size=batch_size)\nvalid_gen = data_gen(X=valid_images, y=valid_labels, batch_size=batch_size)\n\n\n# Number of training and validation steps in an epoch\nnb_train_steps = train_images.shape.num_elements() // batch_size\nnb_valid_steps = valid_images.shape.num_elements() // batch_size\nprint(\"Number of training steps: \", nb_train_steps)\nprint(\"Number of validation steps: \", nb_valid_steps)\n\nwith tf.device('/GPU:0'):\n    for epoch in range(nb_epochs):\n        train_loss, train_acc= [], []\n        for (images, labels) in tfe.Iterator(train_gen):\n            loss, accuracy = model.train_on_batch(images.numpy(), labels.numpy())\n            train_loss.append(loss.numpy())\n            train_acc.append(accuracy.numpy())    \n        print(\"======================= Epoch {} ===================================== \".format(epoch))\n        print(\"train_loss: {:.2f}    train_acc: {:.2f}\".format(np.mean(train_loss), np.mean(train_acc)*100))","execution_count":70,"outputs":[]},{"metadata":{"_uuid":"add9fadec423fa335de3262f1d35e8d8acede128"},"cell_type":"markdown","source":"Simple!!  isn't it? I have tried my best to show that how can you mix tf, keras, eager, and tf.data and make the best use out of it. There are many more things that you can do. \nFor example, you can create your own network using the **tf.layers** API and use all the functionality of keras inside that. In order to do that make sure that your model class inherits the `keras.model`"},{"metadata":{"_uuid":"95192f9d885a955ef46280858d27f9f35286cc8b"},"cell_type":"markdown","source":"## Data Augmentation\nData augmentation is a crucial part of training deep learning models. Writing an efficient augmentation pipeline is itself a big task. Keras provide `ImageDataGenerator` class that can do augmentation on the fly and it's very good but there are certain number of cases when you want to use  some other augmentation techniques.  Although you can write your own augmentation pipeline but I recommend using `imgaug` phython package. It's very flexible and easy to use. Lemme show you a dummy example for the same."},{"metadata":{"_uuid":"d01c35c66f4ff526b20a47ed983a3a66110fe113","collapsed":true,"_cell_guid":"dabcea32-35a6-4c9d-86e5-bbab3fddabaa","trusted":true},"cell_type":"code","source":"# We don't want all the augmentations to be applied on the iamge. The code line below tells that anything isnide iaa.Sometime()\n# will be applied only to random 50% of the images  \nsometimes = lambda aug: iaa.Sometimes(0.5, aug)\n\n# Different types if sugmentation you want to apply on your batch of images\naug_to_apply = [iaa.Fliplr(0.2), sometimes(iaa.Affine(rotate=(10,30))), \n                sometimes(iaa.Multiply((0.5, 1.5)))]\n# Instantiate the augmenter\naug = iaa.Sequential(aug_to_apply, random_order=True, random_state=111)","execution_count":71,"outputs":[]},{"metadata":{"_uuid":"68373f480226e38d23fd6a79fa604cebe3b9c04b","collapsed":true,"_cell_guid":"f6d2469a-95ca-408f-befc-cab73cc67ad8","trusted":true},"cell_type":"code","source":"# I will do the augmentation on the images that I showed above\n\n# Get the numpy array [batch, img.shape]\nsample_images_arr = np.array(sample_images).reshape(len(sample_images), 224, 224, 3)\n\n# Augment the images using augmenter instance we just created in the above cell\nimages_aug = aug.augment_images(sample_images_arr)\nimages_aug = images_aug[...,::-1]","execution_count":74,"outputs":[]},{"metadata":{"_uuid":"fe3d5222b9c68149e415c9ae227f2a5657801de0","_cell_guid":"d4100e55-9a92-4c38-8053-04b97c599757","trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(4,3, figsize=(30,30))\nfor i in range(12):\n    img = images_aug[i]\n    lab = labels_info.loc[labels_info['Label'] == sample_train_images_df.loc[i, 'label']]['Common Name'].values[0]\n    ax[i//3, i%3].imshow(img)\n    ax[i//3, i%3].set_title(lab, fontsize=14)\n    ax[i//3, i%3].axis('off')\n    ax[i//3, i%3].set_aspect('auto')\nplt.show()    ","execution_count":75,"outputs":[]},{"metadata":{"_uuid":"008f472da6291313192635074097c02708925fd8"},"cell_type":"markdown","source":"## Final Thoughts\n* Eager and keras are a great way to go but the documentation at the moment is very unclear. I heard that the it's in process, so fingers crossed.\n* There is a particular functionality `prefecth_to_device()` in the `dataset` API which actually gives the real boost in performance but at the moment there is a bug in that and that's why we use `tfe.Iterator()`\n* IMHO, TF 1.8 was released too early. I have been doing many things since last two weeks and so many things broke. A proper testing of the new functionalities would have been better \n*  One of the most annoying thing with eager as of now is the device placement of TF ops. For eager it is required that the ops are on the same device(either CPU/GPU) to work. That's why we put this line `tf.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)` in the beginning. This is totally insane!! Why? Because it will downside all the boost in performance we get using `tf.data.Dataset`. Device placement is a very expensive operation and is a performance bottleneck. TF guys need to fix these things  ASAP because if they don't, then there will be no eager. No one wants to waste their time on the bugs when they already have pure tf, keras or pytorch to experiment on."},{"metadata":{"_uuid":"f5ea3ca6b71d908893acfc8f2a1c77bba51e9506","collapsed":true,"_cell_guid":"4f701428-4372-4890-a353-19a94e9f6068","trusted":false},"cell_type":"markdown","source":"That's it folks!! I hope you enjoyed every bit of this kernel. **Please upvote if you liked the kernel.** Suggestion/Feedback is welcome.   "},{"metadata":{"_uuid":"42a45bf0d4228ef69939b03ee4bc72f0bb5c9ba7","collapsed":true,"_cell_guid":"aeeb5dad-6b57-47c4-970f-0d319ed8e6cc","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}