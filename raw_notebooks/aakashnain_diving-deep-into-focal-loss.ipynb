{"cells":[{"metadata":{"_uuid":"fe6d727e5bc539704d38677a6495d0112e1e1233"},"cell_type":"markdown","source":"Hello everyone! \nFirst thing first: Enjoying **Kaggle 2.0** after the **Kaggle eclipse**?   üòù Idk if it's just a feeling but it seems like browsing the Kaggle website is much faster. Anyways, today, we are going to deep dive into a few very interesting things. You must be wondering why I choose this dataset **again**? The thing is that this dataset has some nice properties:<br>\n* Highly imbalanced classes\n* Healthcare -> Matters more than any other dataset\n* Very few samples to train on and even fewer to validate on\n* Literally hard to achieve a good combination of **precision** and **recall**\n\nBefore this, I made another [kernel](https://www.kaggle.com/aakashnain/beating-everything-with-depthwise-convolution) where I showed how **Depthwise Convolution** can beat other approaches. Today, I am going to beat that one as well. This notebook contains a lot of new information, from **tips and tricks** to a new **loss function**. Sit tight and read carefully!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport cv2\nimport glob\nimport h5py\nimport shutil\n\nimport imgaug as aug\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport imgaug.augmenters as iaa\n\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Sequential, Model, load_model\nfrom keras.applications.resnet50 import ResNet50, preprocess_input\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout\nfrom keras.layers import Input, Flatten, SeparableConv2D, BatchNormalization\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras.utils import to_categorical\n\nfrom keras import backend as K\nimport tensorflow as tf\n\n\ncolor = sns.color_palette()\n%matplotlib inline\n%config InlineBackend.figure_format=\"svg\"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"## I said it earlier as well, do everything you can to make your results reproducible. It matters!!\n\n# Set the seed for hash based operations in python\nos.environ['PYTHONHASHSEED'] = '0'\n\nseed=1234\n\n# Set the numpy seed\nnp.random.seed(seed)\n\n# Set the random seed in tensorflow at graph level\ntf.set_random_seed(seed)\n\n# Make the augmentation sequence deterministic\naug.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39cf78def296279d011d2de2f68ba9ba700ce15e"},"cell_type":"code","source":"# Defining some paths..as usual!\n\n# data path\ndata_dir = Path('../input/chest_xray/chest_xray')\n\n# path to directory containing train set\ntrain_dir = data_dir / 'train'\n\n# path to directory containing validation set\nval_dir = data_dir / 'val'\n\n# path to the directory containing test set\ntest_dir = data_dir / 'test'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb814a9c056ec9fb3a99a00e0c5ec2f8c00bb483"},"cell_type":"markdown","source":"There are two classes in the dataset **Normal** and **Pneumonia** and for each class, we have some samples stored in the corresponding folder. We will store the names of these images and the corresponding label in a pandas dataframe to make life easy."},{"metadata":{"trusted":true,"_uuid":"66d790d87734138634c7297703d5c8187f0f7b57"},"cell_type":"code","source":"# Get the path to the normal and pneumonia sub-directories\nnormal_cases_dir = train_dir / 'NORMAL'\npneumonia_cases_dir = train_dir / 'PNEUMONIA'\n\n# Get the list of all the images\nnormal_cases = normal_cases_dir.glob('*.jpeg')\npneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n\ntrain_df = []\n\n# Go through all the normal cases. The label for these cases will be 0\nfor img in normal_cases:\n    train_df.append((str(img),0))\n\n# Go through all the pneumonia cases. The label for these cases will be 1\nfor img in pneumonia_cases:\n    train_df.append((str(img), 1))\n\n# Get a pandas dataframe from the data we have in our list \ntrain_df = pd.DataFrame(train_df, columns=['image', 'label'],index=None)\n\n# Shuffle the data \ntrain_df = train_df.sample(frac=1.).reset_index(drop=True)\n\n# How the dataframe looks like?\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d40e08faa7c08a1466589e65295f761c23be860"},"cell_type":"code","source":"# Get the counts for each class\ncases_count = train_df['label'].value_counts()\nprint(cases_count)\n\n# Plot the results \nplt.figure(figsize=(10,4))\nsns.barplot(x=cases_count.index, y= cases_count.values)\nplt.title('Number of cases', fontsize=14)\nplt.xlabel('Case type', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks(range(len(cases_count.index)), ['Normal(0)', 'Pneumonia(1)'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd493f8c17c2c728a467dc8a61b7649b67e8f619"},"cell_type":"markdown","source":"You can see the imbalance between the classes here. The ratio is almost 1:3. Let us take a look at a few samples from each class. After all, visualization is an important part of any data pipeline and data modelling process"},{"metadata":{"trusted":true,"_uuid":"7541e4fbd5b31967acefaecf81c948cbf5110c8e"},"cell_type":"code","source":"# Get few samples for both the classes\npneumonia_samples = (train_df[train_df['label']==1]['image'].iloc[:5]).tolist()\nnormal_samples = (train_df[train_df['label']==0]['image'].iloc[:5]).tolist()\n\n# Concat the data in a single list and del the above two list\nsamples = pneumonia_samples + normal_samples\ndel pneumonia_samples, normal_samples\n\n# Plot the data \nf, ax = plt.subplots(2,5, figsize=(20,6))\nfor i in range(10):\n    img = imread(samples[i])\n    ax[i//5, i%5].imshow(img, cmap='gray')\n    if i<5:\n        ax[i//5, i%5].set_title(\"Pneumonia\")\n    else:\n        ax[i//5, i%5].set_title(\"Normal\")\n    ax[i//5, i%5].axis('off')\n    ax[i//5, i%5].set_aspect('auto')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01dd5a1a56d0e50069665f4e4694a86f184dd5e7"},"cell_type":"markdown","source":"Next, we will define our **augmentations** that we are going to apply to the samples during the training process. At this point, I will stick to just two types of augmentations:\n* Flip right\n* Random rotation"},{"metadata":{"trusted":true,"_uuid":"b8367e8098238cf9596594fea00de0fac0369a23"},"cell_type":"code","source":"# Augmentation sequence \nseq = iaa.OneOf([\n    iaa.Fliplr(), # horizontal flips\n    iaa.Affine(rotate=30)]) # roatation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16b7d5aa7b0e0889a98b302ac0fb2025909d487c"},"cell_type":"code","source":"# some constants(not truly though!) \n\n# dimensions to consider for the images\nimg_rows, img_cols, img_channels = 224,224,3\n# batch size for training  \nbatch_size=16","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6aaa59a8fada614e9a9ccab230b7b04d539824e"},"cell_type":"markdown","source":"Define a training data generator."},{"metadata":{"trusted":true,"_uuid":"c3f6113aeecdf88b11a94b17a723089f19c72c78"},"cell_type":"code","source":"def data_generator(data, batch_size):\n    # Get total number of samples in the data\n    n = len(data)\n    nb_batches = int(np.ceil(n/batch_size))\n\n    # Get a numpy array of all the indices of the input data\n    indices = np.arange(n)\n    \n    # Define two numpy arrays for containing batch data and labels\n    batch_data = np.zeros((batch_size, img_rows, img_cols, img_channels), dtype=np.float32)\n    batch_labels = np.zeros((batch_size,), dtype=np.float32)\n    \n    while True:\n        # shuffle indices for the training data\n        np.random.shuffle(indices)\n            \n        for i in range(nb_batches):\n            # get the next batch \n            next_batch_indices = indices[i*batch_size:(i+1)*batch_size]\n            \n            # process the next batch\n            for j, idx in enumerate(next_batch_indices):\n                img = cv2.imread(data.iloc[idx][\"image\"])\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                img = seq.augment_image(img)\n                img = cv2.resize(img, (img_rows, img_cols)).astype(np.float32)\n                label = data.iloc[idx][\"label\"]\n                \n                batch_data[j] = img\n                batch_labels[j] = label\n            \n            batch_data = preprocess_input(batch_data)\n            yield batch_data, batch_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5abb6219a827a00f9f462d660becd97b771040f"},"cell_type":"code","source":"# training data generator \ntrain_data_gen = data_generator(train_df, batch_size)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce074e305cad4b95bf7dbb42b45b90a66c836445"},"cell_type":"markdown","source":"The **validation** set as well as the **test** set is very small. Hence we will load these two directly in the memory. This will be a one-time process but this will save a lot of time during the validation and testing process."},{"metadata":{"trusted":true,"_uuid":"653765217e02b8c4459f95ec4b4b716cc0532acb"},"cell_type":"code","source":"def read_images(images, label):\n    \"\"\"\n    This function read images from a list of the given\n    images. For each image it records a corresponding label.\n    \n    Args:\n        images: list of all images\n        label(int): label for these images\n    \n    Returns:\n        data: numpy array containing pre-processed images. The pre-processing\n              function is provided by keras\n        labels: numpy array of corresponding labels\n    \"\"\"\n    data = []\n    for img in images:\n        img = cv2.imread(str(img))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (img_rows, img_cols)).astype(np.float32)\n        data.append(img)\n    \n    labels = [label]*len(data)\n    data = np.array(data).astype(np.float32)\n    data = preprocess_input(data)\n    return data, labels\n\n########################################################################################\n\ndef prepare_data(data_dir):\n    \"\"\"\n    This function can be used to prepare the \n    validation and test datasets. \n    \n    Args:\n        data_dir: A pathlib object pointing to validation/test directory\n    \n    Returns:\n        data: numpy array of all images\n        labels: numpy array of corresponding labels\n    \"\"\"\n    normal_cases_dir = data_dir / 'NORMAL'\n    pneumonia_cases_dir = data_dir / 'PNEUMONIA'\n\n    # Get the list of all the images\n    normal_cases = list(normal_cases_dir.glob('*.jpeg'))\n    pneumonia_cases = list(pneumonia_cases_dir.glob('*.jpeg'))\n    print(f\"Found {len(normal_cases)} normal cases and {len(pneumonia_cases)} pneumonia_cases\")\n    \n    # process class-wise\n    normal_cases_data, normal_cases_labels = read_images(normal_cases, 0)\n    pneumonia_cases_data, pneumonia_cases_labels = read_images(pneumonia_cases, 1)\n    \n    # sanity-check\n    assert len(normal_cases_data) == len(normal_cases_labels), \"You had one job!\"\n    assert len(pneumonia_cases_data) == len(pneumonia_cases_labels), \"You can't get it right, can you?\"\n    \n    data = np.vstack((normal_cases_data, pneumonia_cases_data))\n    labels = np.array((normal_cases_labels + pneumonia_cases_labels)).astype(np.float32)\n    \n    return data, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"865535be67a4a6f44d7d9062dbeec2bea17062e1"},"cell_type":"code","source":"# Prepare validation data \nvalidation_data, validation_labels = prepare_data(val_dir)\nprint(f\"Number of validation images: {len(validation_data)} and labels: {len(validation_labels)}\")\nprint(validation_data.shape, validation_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89643228b18a1cf47c563cf46c58d9d06c2835e6"},"cell_type":"code","source":"# Prepare test data\ntest_data, test_labels = prepare_data(test_dir)\nprint(\"Number of samples in test data: \", len(test_data))\nprint(test_data.shape, test_labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76e9688b4dba1a84586761fa4152e51b5841f53c"},"cell_type":"markdown","source":"You can see that we have only **16** samples in the validation set. This sample size is so small that getting good results on this doesn't necessarily mean that your model is good enough. Now, one might argue that you can simply switch **test** and **validation** set and get a bigger validation set. I would say that doing so is much worse than validating on small samples. Why? Becuase, in that case, your test set will be very small and taking that performance measure as a reference for validating goodness of model can turn out to be very bad.  \n\nA good strategy would be to remove some samples from test set permanently and adding them in the validation set, For example, instead of having 624 samples, you can simply have 500 test samples and add the extra 124 samples to your validation set. I am not going to do it here but you can try later on."},{"metadata":{"_uuid":"fabb6eb17c28084cefc6cff4c65ec4a3173530fd"},"cell_type":"markdown","source":"For training, we will use **ResNet50** as our base model and we will do transfer learning on top of it. <br>\n**Beware:**\n* Freezing layers in any pre-trained network in Keras doesn't freeze all parts of a **BatchNormalization** layer. If your batch size is small, then it can be a huge problem as your validation loss will explode or your validation accuracy will be very low as compared to training accuracy. **This is not a typical overfitting case.** It's just that BN is messing with your training. In order to make sure that BN runs in **inference** mode, you have to set the learning phase as well.  \n* Some people might say that the keras implementation of batch normalization is broken. I disagree with that completely. The training mode and inference mode is very different from any other. Hence *freezing the layer* has a different context when it comes to batch normalization. Check [this issue](https://github.com/keras-team/keras/pull/9965) for more details."},{"metadata":{"trusted":true,"_uuid":"baf9c04376cffa827b5e816dc89f51196ccdf8db"},"cell_type":"code","source":"# Get fine-tuning/transfer-learning model\ndef get_fine_tuning_model(base_model, top_model, inputs, learning_type):\n    if learning_type=='transfer_learning':\n        print(\"Doing transfer learning\")\n        K.set_learning_phase(0)\n        base_model.trainable = False\n        features = base_model(inputs)\n        outputs = top_model(features)\n    else:\n        print(\"Doing fine-tuning\")\n        base_model.trainable = True\n        features = base_model(inputs)\n        outputs = top_model(features)\n    return Model(inputs, outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6dbcc58fed83583be408489b09e706e3f8bf9379"},"cell_type":"code","source":"# Get the base model\nbase_model = ResNet50(input_shape=(img_rows, img_cols, img_channels), \n                       weights='imagenet', \n                       include_top=False, \n                       pooling='avg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"587769c97f1b0e8d453582ab8acbfc2fd473fe29"},"cell_type":"code","source":"# Define a top model: extra layers that we are going to add on top of our base network\nfeature_inputs = Input(shape=base_model.output_shape, name='top_model_input')\nx = Dense(50, activation='relu', name='fc1')(feature_inputs)\nx = Dropout(0.5,name='drop')(x)\noutputs = Dense(1, activation='sigmoid', name='fc2')(x)\ntop_model = Model(feature_inputs, outputs, name='top_model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c200c86b404ee1992a5340e5dfbd83fb65ad0e3"},"cell_type":"code","source":"# get model for tranfser learning\ninputs = Input(shape=(img_rows, img_cols, img_channels))\nmodel = get_fine_tuning_model(base_model, top_model, inputs, \"transfer_learning\")\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"133f6f70ab783290c5b2808b1075cc861ce077d2"},"cell_type":"markdown","source":"We are going to define a new loss function today. **Focal loss** was first introduced in **[RetinaNet](https://arxiv.org/abs/1708.02002)**. This loss function is highly useful when you have highly imbalanced classes. I highly recommend reading that paper if you haven't got a chance yet."},{"metadata":{"trusted":true,"_uuid":"8b0b54d19b081cf7a55f804cbb155c4468d297e1"},"cell_type":"code","source":"# focal loss \ndef focal_loss(alpha=0.25,gamma=2.0):\n    def focal_crossentropy(y_true, y_pred):\n        bce = K.binary_crossentropy(y_true, y_pred)\n        \n        y_pred = K.clip(y_pred, K.epsilon(), 1.- K.epsilon())\n        p_t = (y_true*y_pred) + ((1-y_true)*(1-y_pred))\n        \n        alpha_factor = 1\n        modulating_factor = 1\n\n        alpha_factor = y_true*alpha + ((1-alpha)*(1-y_true))\n        modulating_factor = K.pow((1-p_t), gamma)\n\n        # compute the final loss and return\n        return K.mean(alpha_factor*modulating_factor*bce, axis=-1)\n    return focal_crossentropy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b18cde5bf65ab01344770190746abd19301d7b17"},"cell_type":"code","source":"# compile the model and check it \noptimizer = RMSprop(0.0001)\nmodel.compile(loss=focal_loss(), optimizer=optimizer, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0e8eecf9654361562ae6fca065d56df37d086a5"},"cell_type":"code","source":"# always use earlystopping\n# the restore_best_weights parameter load the weights of the best iteration once the training finishes\nes = EarlyStopping(patience=5, restore_best_weights=True)\n\n# checkpoint to save model\nchkpt = ModelCheckpoint(filepath=\"model1\", save_best_only=True)\n\n# number of training and validation steps for training and validation\nnb_train_steps = int(np.ceil(len(train_df)/batch_size))\n\n# number of epochs \nnb_epochs=100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73d831b1d3f7dc54c1f6b91dd3017b108564c706"},"cell_type":"code","source":"# train the model \nhistory1 = model.fit_generator(train_data_gen, \n                              epochs=nb_epochs, \n                              steps_per_epoch=nb_train_steps, \n                              validation_data=(validation_data, validation_labels),\n                              callbacks=[es,chkpt],\n                              class_weight={0:1.0, 1:0.33})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7188a04b3c45d4c2e8f5456dcaf08f34696985c5"},"cell_type":"code","source":"# plot the model loss and accuracy\ntrain_loss = history1.history['loss']\ntrain_acc = history1.history['acc']\n\nvalid_loss = history1.history['val_loss']\nvalid_acc = history1.history['val_acc']\n\nx = [(i+1) for i in range(len(train_loss))]\n\nf,ax = plt.subplots(1,2, figsize=(12,5))\nax[0].plot(x, train_loss)\nax[0].plot(x, valid_loss)\nax[0].set_title(\"Loss plot\")\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"loss\")\nax[0].legend(['train', 'valid'])\n\n\nax[1].plot(x, train_acc)\nax[1].plot(x, valid_acc)\nax[1].set_title(\"Accuracy plot\")\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"acc\")\nax[1].legend(['train', 'valid'])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b84ce762e3ebb118f690903f23c9c9630f545154"},"cell_type":"code","source":"# evaluate on test set\ntest_loss, test_acc = model.evaluate(test_data, test_labels, batch_size=batch_size)\nprint(\"Test loss: \", test_loss)\nprint(f\"Test set accuracy: {test_acc*100:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1911f04289133389ba6cbe1ff49b26711463fc76"},"cell_type":"code","source":"# Get the predictions on test set\npreds = model.predict(test_data, batch_size=16)\npreds = np.squeeze((preds > 0.5).astype('int'))\norig = test_labels.astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e987af4cae9ef3927a9fde0b501956860665e016"},"cell_type":"code","source":"# Get the confusion matrix\ncm  = confusion_matrix(orig, preds)\nplt.figure()\nplot_confusion_matrix(cm,figsize=(10,5), hide_ticks=True, cmap=plt.cm.Blues)\nplt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15e65224148841ac7aee97acd20a70154806e249"},"cell_type":"code","source":"# Calculate Precision and Recall\ntn, fp, fn, tp = cm.ravel()\n\nprecision = tp/(tp+fp)\nrecall = tp/(tp+fn)\n\nprint(\"Recall of the model is {:.2f}\".format(recall))\nprint(\"Precision of the model is {:.2f}\".format(precision))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5d943aa4ee07a5ecb9b8ddd0e9b010a1b12f755"},"cell_type":"markdown","source":"You can see that this is the **best** result achieved so far on this dataset. The model has a **high recall** as well as **very good** precision values. It is much better than my previous model(check for the link to the previous kernel at the top of the notebook)"},{"metadata":{"trusted":true,"_uuid":"2c7c546e828404f92d576bf3fc5abab4717cc277"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}